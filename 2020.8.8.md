---
title: 2020.8.8

renderNumberedHeading: true
grammar_cjkRuby: true
---

# Inception
## Inception v1
 * 避免过拟合
 * 提高计算性能：采用1x1卷积核来进行降维。
 * 例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据的大小为100x100x256。其中，卷积层的参数为5x5x128x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据的大小仍为100x100x256，但卷积参数量已经减少为1x1x128x32 + 5x5x32x256，大约减少了4倍。
* 参考资料
  * [inception v1基本结构](https://www.cnblogs.com/dengshunge/p/10808191.html)


  
## Inception v2/v3
* 大kernel分解为多个小kernel的累加
* 将对称的conv计算分解为非对称的conv计算
例如：是将一个3x3的conv分解为了两个分别为1x3与3x1的conv计算。这样同样可以只使用约（1x3 + 3x1) / (3x3) = 67%的计算开销。
但并不是所有情况下的非对称卷积运算都可以减少计算开销。
* 增加分类层
   在extra loss的全连接层里添加了BN或者dropout层   
  * BN 
    Batch Normalization，求取输入值的均值与方差，将输入值进行标准化后，训练yi=ax+b 的两个参数，输出通过这个线性变换得到新的值。
	可以缓解梯度消失和梯度爆炸的问题。
	在CNN中用BN：输入[m,f,p,q]，则Mini-batch=m*q*p，对每一特征图，得到一组参数。
  * dropout  
    在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。
	在训练的时候，按一定的概率（retaining probability）p 来对weight layer 的参数进行随机采样，将这个子网络作为此次更新的目标网络。
	可以缓解过拟合。
* 参考资料
  * [Inception v2/v3 基本结构](https://www.jianshu.com/p/d6ca52105cb5)
  * [Inception 2结构](https://blog.csdn.net/zziahgf/article/details/82801077)
  * [BN原理与使用](https://blog.csdn.net/weixin_43937316/article/details/99573134)
  * [对dropout的理解](https://blog.csdn.net/u012702874/article/details/45030991?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param)
  * [理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)
  * [loss 层](https://blog.csdn.net/zxyhhjs2017/article/details/78939591)
  * [为什么非对称卷积减少了运算量](https://www.zhihu.com/question/270055683/answer/351817237)

# pytorch
(在一边看例程一边看官方文档，熟悉函数）
## 函数
* nn.conv2d 
  参数：输入通道数、输出通道数、卷积核大小、stride（默认1）、padding（默认0）
* nn.dropout2d
  参数：概率p
  输出：对每个通道按照概率p置为0
 * nn.dropout
   参数：概率p
   输出：对所有元素中每个元素按照概率p更改为零
  * nn.Linear
    参数：输入大小、输出大小、是否偏置
  * nn.functional.max_pool2d
    池化
  * torch.flatten
    展开
  * nn.functional.relu
    激活函数，小于0为0，大于0不变
 * nn.functional.log_softmax
   log(softmax())
  ## 参考资料
  * [官方文档](https://pytorch.org/docs/stable/nn.functional.html)
  * [pytorch历程](https://github.com/pytorch/examples)
   
   # Yolov4
 尝试先按照readme的介绍跑通demo，下载了权重文件，但运行时报错，找不到指定文件，改成绝对路径也不行。
 
 # 问题
 * Inception具体结构还不够清楚，明天去看具体代码
 * 在师兄给的账号下部署模型前需要建虚拟环境吗？
    
      


