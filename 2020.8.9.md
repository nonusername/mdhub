---
title: 2020.8.9
renderNumberedHeading: true
grammar_cjkRuby: true
---

# Inception
昨天主要是不明白BN层和dropout层是怎样配合其他层的，看了pytorch实现inception的代码之后明白了。BN接收卷积层的输出，处理数据后交给激活函数，dropout层用在最后的全连接层之前。
看后面几个网络之后更清楚了。
# ResNet
## 功能
解决梯度消失、梯度爆炸的问题
## 结构
有2层结构的和三层结构的，2层：

![2层的ResNet](./attachments/1596938813543.drawio.html)


   在实现残差网络时，对于其中的输入、输出通道数目不同的情况可以使用1x1的Conv使得输入、输出通道数目相同。
 ## 变体
 WRN：加宽（即增加Conv的Output channels数目）、增加dropout层（解决加宽后参数过多带来过拟合的问题）。比起加深ResNet性能更好。
# DenseNet
相比ResNet，DenseNet互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。
DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。
![DenseBlock+Transition](https://pic4.zhimg.com/80/v2-ed66515594a04be849a8cee707cb83bf_720w.jpg)

# AlexNet
## 结构
![Alexnet](https://imgconvert.csdnimg.cn/aHR0cDovL3BpY3R1cmUucGlnZ3lnYWdhLnRvcC9BbGV4TmV0L0FsZXhOZXQucG5n?x-oss-process=image/format,png)
不算输入层，AlexNet网络结构共有8层，前面5层是卷积层，后面3层是全连接层，最后一个全连接层的输出传递给一个1000路的softmax层，对应1000个类标签的分布。
由于AlexNet采用了两个GPU进行训练，因此，该网络结构图由上下两部分组成，一个GPU运行图上方的层，另一个运行图下方的层，两个GPU只在特定的层通信。例如第二、四、五层卷积层的核只和同一个GPU上的前一层的核特征图相连，第三层卷积层和第二层所有的核特征图相连接，全连接层中的神经元和前一层中的所有神经元相连接。
* 第一、二层：卷积-->ReLu-->池化-->归一化
* 第三、四层：卷积-->ReLu
* 第五层：卷积-->ReLu-->池化
* 第六、七层：全连接-->ReLu-->Dropout
* 第八层：第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出1000个float型的值，就是预测结果。
 ## 特点
 * 多个GPU
 * 局部归一化
 * 重叠池化
  
  # VGG
  VGGNet可以看成是加深版本的AlexNet，都是由卷积层、全连接层两大部分构成。
  对AlexNet的改进：采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核。对于给定的感受野，采用堆积的小卷积核是优于采用大的卷积核。
    
  # LeNet5
  第三层（卷积层）比较特别，与上一个池化层是部分连接。
  ![LeNet5第三层连接方式](./images/图片1.png)
  
  例如第一列表示C3层的第0个特征图（feature map）只跟S2层的第0、1和2这三个feature maps相连接

# Pytorch
## 函数
* nn.functional.nll_loss
  参数：output、target
* optim.Adadelta
  参数：模型参数、rho（梯度平方的均值乘的系数，默认0.9）、eps（加在分母中，防止分母为0，默认1e-6）、learning rate（默认1.0）、权重衰减（默认0，权重衰减的作用是防止过拟合）
 * optim.lr_scheduler.StepLR
   参数：优化器、步数、衰减gamma
   每隔一定步数（或者epoch）就减少为原来的gamma分之一。
  * torch.cat
    参数：待拼接的tensor、0（按行拼）或1（按列拼）

  

