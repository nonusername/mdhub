---
title: 小结2
renderNumberedHeading: true
grammar_cjkRuby: true
---

# 
* 计算复杂度，处理速度（若有多个滤波器，分别学习；傅里叶域；线性化；稀疏矩阵；）
* 遮挡、光照变化
* 边缘效应
* 信息不足、特征利用不足（使用神经网络中某些层产生的特征图；使用色彩直方图对CF的方法做一些补充）
* 存储（每隔一段时间或存储量达到上限，丢弃权重低的数据；有一些方法不需要记忆sample信息，只需要根据之前的参数对模型进行更新）
* 尺度变化（先检测位置，再检测尺度；学习尺度滤波）
* 更新
* 离线/在线学习
* 精度（离散化为连续）
* target外观模型（使用后几帧的预测结果作为training sample；每隔一段时间对模型进行更新）

#
## 相关过滤类
KCF、MUSTER、DSST、ECO、SRDCF、C-COT、STAPLE、LMCF
* KCF 
* MUSTER
* DSST
* ECO
* SRDCF
 (以上在小结1）
 
* C-COT
    使用了更加多的信息：输入包括原始图像（RGB）、CNN的第一层与最后一层产生的特征图。
	构造了一个插值运算、提高精度：把离散的像素点，映射到了一个连续域上，因此精度得到了提高。使得多种分辨率的特征图能够被有效利用（而不是简单的resample)。
	计算复杂度：与基本的CF类似，映射到傅里叶域中。
	                      截取部分傅里叶做近似处理，截取的这部分（Kd)越大，则精度越高，但计算量就越大。经过变换后，可得一组稀疏、线性的等式。
						  使用CG法提高运算效率。
						  在feature point tracking中使用单通道特征图并令惩罚系数为常数。
    过程：获取位置：训练获得滤波器f，从region of interest中获取特征图，先做grid search获取大致的位置，再在得分最高处做更细致的处理。
	           目标检测：从训练好的网络中获取不同分辨率的特征图，每一帧中只取一个training sample，限制最大的sample存储量，并替换掉w最小的sample。在第一帧，做100次迭代来估计滤波器系数，此后每帧5次迭代。
			   （训练好滤波器后，不在检测过程中更新，并且训练过程中就限制sample数量）
			   （这里训练好的滤波器应该是用追踪物本身的视频训练的？）
			   
* STAPLE
       target 外观描述：CF过于依赖空间位置信息，色彩统计信息可以更好地处理形状的变化，但对于光照的影响过于敏感。但这两者之间是互补的，所以在STAPLE中结合二者（CF与色彩直方图）。以往从第一帧中去学习target外观的方法并不是很有效（其中一种简单的解决方法是将后几帧的预测结果作为training data去更新模型，但这样会造成误差的累积）。
       计算复杂度:   CF与色彩直方图的结合是线性的。
							 衡量当正确位置为p时选择q的代价，这个计算耗费大，而CF衡量损失的函数所带来的计算量是小的。
							 独立地学习CF和色彩直方图的参数。
							 在学习色彩直方图时，对前景和后景分别地进行学习。
		过程：模型参数是会根据上一帧的参数进行更新的，存储的仅是参数，而不是每一个sample的具体信息，因此不会有存储的问题。先做translation，再做scale，并且仅在上一帧的附近位置做
	   
* LMCF

## 神经网络类
ATOM、MDNET、CFNET、SiamMask、SiameseFC、SiameseRPN、SiameseRPN++、SiamRCNN
* ATOM
  论文中，classification是通过判断前景与背景进行部分目标信息的获取（图像坐标），estimation是找到目标更为完整的状态，并且被简化为寻找能够最佳描述target的矩形框（当target只是在和相机平面平行的面上移动时，其实没有必要区分estimation和classification，但大多数情况下，都会发生视角等的变化）。
  计算复杂度：在目标辨别部分使用Conjugate-Gradient-based strategy，确保可以实时检测。
  精度：在线学习分类模型
             hard negative mining:寻找一些容易判断为正样本的负样本进行训练。
  过程：离线学习estimation部分，在线追踪时权重不再变化（使用IoU的方法，这个方法需要较多的信息，不适合仅用一帧的数据对其调整）。
              
* MDNET
    训练CNN做追踪的难点：在图片序列中，不同的target会有不同的类别、移动模式、外观，而且tracking算法受到遮挡、变形、光线条件、模糊等的影响。二七二在一个序列中被认为是target的，在另一个序列中就是一个背景。
    有提前对特定目标进行离线训练的、在线训练却因缺少训练数据所以效果不好的。

    MDNET基于CNN，每一个视频被视作一个单独的domain，网络具有的每个domain特有的层都有单独的branch输出，并且共享信息。每一个domain都是单独训练的，并且在每一次迭代中更新共享层。
另一个是提出了一个有效的online tracking framework.当给出一个测试序列时，所有用在训练阶段的二分类层的branches被去掉，换成新的single branch，来计算target score。新的分类层和共享层中的全连接层会在tracking期间微调来适应新的序列。online update会学习长期和短期的目标外观。

  网络比较小，因为：1、tracking只是为了区分2类；2、过深的网络会丢失信息；3、target通常很小，所以输入也不大，不需要很深的网络；4、更加适合online

算法的目标是训练一个 multi-domain CNN 以在任何 domain 辨别 target 和 background。这并非很直观，因为来源不同 domain的 train data 拥有不同的 target 和 background 的定义。但是，这其中仍然存在着一些共同的属性，如：对光照变化，运动模糊，尺寸变化的鲁棒性等等。为了提取出满足上述属性的特征，作者通过 multi-domain learning framework，从 domain-specific 的信息中分离出 domain-independent 的信息。

Long-term update 是按照常规间隔后进行更新。short-term updates 当出现潜在的跟踪失败的时候进行更新.

hard negative mining:解决负样本效率低的问题

bounding box regression: 提高定位精度

算法：预训练好的模型w1-w5以及初始的target位置x1。
           随机初始化w6，训练一个bounding box regression model(只在第一帧训练）
		   获取正负样本，并使用它们来更新w4-w6
		   Ts与Tl计入1
		   重复：
		           找到target候选样本
				   找到得分最高的样本
				   若此分数>0.5，获取正负训练样本，Ts、Tl计入t，并且在超出容量时舍弃数字最小的（最早的），使用bounding box regression 调整target位置。
				   若此分数<0.5 ，使用短期记忆更新w4-w6，若t mod 10=0 ,使用长期记忆的正样本和短期记忆的负样本更新w4-w6。
			结束。
			
* CFNET
    例子少，缺少先验知识; 最简单的方法是，忽视缺少的先验知识并使用一个预训练好的CNN。但缺乏的训练数据和大量的参数，以及SGD的运算复杂度，使得这件事难以完成。
一个可能的解决方案就是不要用在线学习，rencent works have focused on learning deep embeddings that can be used as universal object descriptors。这些方法使用了孪生网络CNN，离线学习。
但是，使用fixed metric 去比较外观（？？？Siamese再看一眼）
另一种解决方法，就是使用别的在线学习算法，例如CF。
问题在于，如何结合CF在线学习的效率与离线学习的CNN的辨别能力。目前有一些方法，仅仅将CF用于CNN预训练后得到的特征。end-to-end是比部件单独训练更好的方法。

CFNET：将相关过滤转换成深度神经网络中的differentialble layer。

tracking: 不同于原本的孪生网络，CFNET compute a new template in each
 frame and then combine this with the previous template in a moving average.
 
 Siam相关：
 
* SiameseFC
  现有方法与问题：限制了学习空间；使用卷积网络时，由于需要用SGD，造成运行速度慢。
                             由于要track的可能是任何的一个物体，因此不太可能有收集好、训练好的数据来训练特定的检测器。常见方法是从要检测的视频中学习物品的外观，但是这导致了只能学习简单的模型。有一些人选择用不同但相关的数据训练深度卷积网络，要么是使用神经网络的内部表现作为特征，要么使用SGD去微调神经网络。但前者没有利用好端到端学习的优势，后者无法用于实时跟踪。
	
SIAMESEFC：离线训练神经网络去解决一个similarity learning problem，再应用于在线。

deep similarity learning : 学习一个函数，计算候选块和样例的相似度，找出得分最高的。函数形式选择深度卷积网络，并添加孪生结构。孪生结构会对两个输入做转换，然后合并。

fully convolutional siamese architecture:（这边一部分讲的是检测）
使用了互相关层

training with large search images:v是计算得分，y是实际标签。输入的两张图是从同一个视频中获取的样例和以目标为中心的search images 。

* SiameseRPN
  现有的方法与问题：无法达到实时检测的速度，大部分使用了孪生网络的精度和稳定性都不足

Siamese: 因为没有预先定义好的类别，所以需要template branch记录目标外观信息。
离线学习大量数据，不需要做尺度检测。

网络包括2个部分，一部分是feature extraction,另一部分是 proposal generation。特别的，在后半部分有2个分支，一个是区分前景后景的，一个是做位置优化的。

siamese feature extraction subnetwork:使用了去掉conv2和conv4的AlexNet。template branch输入含目标的历史帧，detection branch输入当前帧。

Region proposal subnetwork:RPN的部分

training: 利用Imagenet预训练Siamese，再用视频训练整个网络。因为前后两帧的相同物体变化不会太大，所以减小了anchor。样本的选取。

tracking as one-shot detection:
在inference阶段，孪生网络中的template branch移除，来提高效率。target patch是从第一帧来的，detectio kernel是计算好的（来自第一帧）。

inferance:detectio kernel是来自第一帧的template branch的输出，并且以后不会变。
计算得到分值最高的M个proposal，可以进一步提高坐标精度。4.3讲了2中选择select proposals的简化方法。

* SiameseRPN++
  许多方法用的都是基于ALexNet，并尝试使用复杂的结构。一般使用孪生网络的方法都有精度问题而且不能利用深度网络提取的特征.核心原因是缺乏严格的平移不变性。

SIAMESERPN++：深度神经网络可以打破平移不变性的限制带来的误差；一个简单有效、打破空间平移不变性限制的采样方法；提出了一个用于实现互相关的结构；

直接套用深度网络并不能获得预习的提升。

 Analysis on Siamese Networks for Tracking：
 Siamese 对于相似度的计算公式有2个内在限制：对称性、平移不变性。
 具体的说，导致了Siamese无法使用深度神经网络的原因之一是padding会打破平移不变性（大部分网络都不可避免地需要padding)。
* SiamMask
* SiamRCNN
